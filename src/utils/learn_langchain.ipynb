{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Wiede\\OneDrive - Queen Mary, University of London\\Documentation\\Projects\\HCockpit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_project_root(start_dir=\".\"):\n",
    "    current_dir = os.path.abspath(start_dir)\n",
    "    while True:\n",
    "        if os.path.exists(os.path.join(current_dir, \".project_root\")):\n",
    "            return current_dir\n",
    "        \n",
    "        parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "        \n",
    "        if parent_dir == current_dir:\n",
    "            raise FileNotFoundError(\"Project root not found.\")\n",
    "        \n",
    "        current_dir = parent_dir\n",
    "\n",
    "project_root = find_project_root()\n",
    "os.chdir(project_root)\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get info from https://api.smith.langchain.com: ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'ËøúÁ®ã‰∏ªÊú∫Âº∫Ëø´ÂÖ≥Èó≠‰∫Ü‰∏Ä‰∏™Áé∞ÊúâÁöÑËøûÊé•„ÄÇ', None, 10054, None)))\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Hi!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "  \n",
    "image_base64 = encode_image(\"./docs/figures/s-3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image illustrates an automotive AI system designed to assist a driver. Below is a breakdown of the elements present in the image:\n",
      "\n",
      "1. **Center Image (Driver's View)**: This shows a simulated first-person perspective from inside a car. The driver's hand is on the steering wheel, and a sun glare is visible, suggesting visibility challenges. There is an icon that suggests the system is monitoring the surroundings.\n",
      "\n",
      "2. **AI Process Flow**:\n",
      "    - **H.Copilot**: Depicted as a cartoonish figure wearing goggles, signifying the AI 'co-pilot'.\n",
      "    - **Sensors and Tools**: Labeled lines indicate tools and sensors feeding information into the H.Copilot system:\n",
      "        - `tools.obs_sensor()`\n",
      "        - `tools.obs_hctrl()`\n",
      "        - `tools.obs_hctr()`\n",
      "        - `tools.set_cam_to()`\n",
      "        - `tools.speech()`\n",
      "\n",
      "3. **Right Image (External View)**: This shows a streetscape with a car approaching from the left, implying a potential hazard for the driver.\n",
      "\n",
      "4. **Output Panel**:\n",
      "    - **Perception**: Describes what the AI detects:\n",
      "        - A car coming from the left.\n",
      "        - The driver is not paying attention to the car.\n",
      "        - The driver intends to go straight.\n",
      "        - The sun is dazzling.\n",
      "        \n",
      "    - **Plan**: Suggests that the driver should pay close attention to the vehicle indicated by a variable `{v-\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "            \"detail\": \"auto\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This image appears to describe a scenario involving an autonomous driving system, likely guided by some sort of AI co-pilot, here referred to as \"HCopilot\". The process can be summarized as follows:\\n\\n1. **Observation**:\\n   - The system uses sensors (tools.obs_sensor) to observe the surroundings. In this instance, it detects:\\n     - A car approaching from the left.\\n     - The driver is not paying attention to the approaching car.\\n     - The driver\\'s intention to go straight.\\n     - The sun is causing dazzling light, which could impair visibility.\\n\\n2. **Perception**:\\n   - The perception module of HCopilot interprets the sensor data and infers the current scenario:\\n     - A car is coming from the left.\\n     - The driver is not attentive to the approaching car.\\n     - The driver intends to drive straight.\\n     - The sun\\'s glare adds to the challenge of driving safely.\\n\\n3. **Planning**:\\n   - Based on the perception, HCopilot formulates a plan.\\n   - The plan suggests that the driver should pay close attention to the vehicle approaching from the left (indicated by `{v-100}`).\\n\\n4. **Control**:\\n   - HCopilot then issues control commands:\\n     - `set_cam_to(v-100)`: This likely adjusts the camera or sensor to focus on the vehicle coming from the left.\\n     - `text2sound(watch out the car from left)`: This command converts text to an audible alert to warn the driver about the approaching car.\\n\\nThe accompanying visual elements (images) seemingly support the scenario:\\n- The first image shows the driver\\'s perspective with the sun glaring, possibly causing visibility issues.\\n- The second image shows a car approaching from the left side, which matches the scenario described by the AI.\\n\\nIn essence, this system aims to enhance driving safety by alerting the driver to potential hazards that they might not have noticed due to distractions or environmental factors such as sun glare.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    {\"type\": \"text\", \"text\": \"{input}\"},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "            # \"url\": \"{img_url}\",\n",
    "            \"detail\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_openai = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": messages\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical expert.\"),\n",
    "    HumanMessage(content=messages),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"What is in this image?\"})\n",
    "# chain.invoke({\"input\": \"What is in this image?\",\n",
    "#               \"img_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "#               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, LangChain accepts image prompts. The `ImagePromptTemplate` class mentioned in the context is designed for creating prompts for a multimodal model with image inputs. This template allows the creation of new models by parsing and validating input data specifically designed to handle ImageURLs, confirming that LangChain can work with image-based inputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "input = \"Does LangChain accept image prompt?\"\n",
    "res = retrieval_chain.invoke({\"input\": input})\n",
    "\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Round Chat Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain\\n\\n\\nCore\\n\\n\\nCommunity\\n\\n\\nExperimental\\n\\n\\nText splitters\\n\\n\\nai21\\n\\n\\nairbyte\\n\\n\\nanthropic\\n\\n\\nastradb\\n\\n\\nchroma\\n\\n\\ncohere\\n\\n\\nelasticsearch\\n\\n\\nexa\\n\\n\\nfireworks\\n\\n\\ngoogle-genai\\n\\n\\ngoogle-vertexai\\n\\n\\ngroq\\n\\n\\nibm\\n\\n\\nmistralai\\n\\n\\nmongodb\\n\\n\\nnomic\\n\\n\\nnvidia-ai-endpoints\\n\\n\\nnvidia-trt\\n\\n\\nopenai\\n\\n\\npinecone\\n\\n\\npostgres\\n\\n\\nrobocorp\\n\\n\\ntogether\\n\\n\\nupstage\\n\\n\\nvoyageai\\n\\n\\nPartner libs\\n\\nai21\\nairbyte\\nanthropic\\nastradb\\nchroma\\ncohere\\nelasticsearch\\nexa\\nfireworks\\ngoogle-genai\\ngoogle-vertexai\\ngroq\\nibm\\nmistralai\\nmongodb\\nnomic\\nnvidia-ai-endpoints\\nnvidia-trt\\nopenai\\npinecone\\npostgres\\nrobocorp\\ntogether\\nupstage\\nvoyageai\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle Menu\\n\\n\\n\\nPrev\\nUp\\nNext\\n\\n\\n\\nlangchain_core.prompts.image.ImagePromptTemplate\\nImagePromptTemplate\\nImagePromptTemplate.input_types\\nImagePromptTemplate.input_variables\\nImagePromptTemplate.metadata\\nImagePromptTemplate.output_parser\\nImagePromptTemplate.partial_variables\\nImagePromptTemplate.tags\\nImagePromptTemplate.template\\nImagePromptTemplate.abatch()\\nImagePromptTemplate.abatch_as_completed()\\nImagePromptTemplate.aformat()\\nImagePromptTemplate.aformat_prompt()\\nImagePromptTemplate.ainvoke()\\nImagePromptTemplate.assign()\\nImagePromptTemplate.astream()\\nImagePromptTemplate.astream_events()\\nImagePromptTemplate.astream_log()\\nImagePromptTemplate.atransform()\\nImagePromptTemplate.batch()\\nImagePromptTemplate.batch_as_completed()\\nImagePromptTemplate.bind()\\nImagePromptTemplate.config_schema()\\nImagePromptTemplate.configurable_alternatives()\\nImagePromptTemplate.configurable_fields()\\nImagePromptTemplate.construct()\\nImagePromptTemplate.copy()\\nImagePromptTemplate.dict()\\nImagePromptTemplate.format()\\nImagePromptTemplate.format_prompt()\\nImagePromptTemplate.from_orm()\\nImagePromptTemplate.get_graph()\\nImagePromptTemplate.get_input_schema()\\nImagePromptTemplate.get_lc_namespace()\\nImagePromptTemplate.get_name()\\nImagePromptTemplate.get_output_schema()\\nImagePromptTemplate.get_prompts()\\nImagePromptTemplate.invoke()\\nImagePromptTemplate.is_lc_serializable()\\nImagePromptTemplate.json()\\nImagePromptTemplate.lc_id()\\nImagePromptTemplate.map()\\nImagePromptTemplate.parse_file()\\nImagePromptTemplate.parse_obj()\\nImagePromptTemplate.parse_raw()\\nImagePromptTemplate.partial()\\nImagePromptTemplate.pick()\\nImagePromptTemplate.pipe()\\nImagePromptTemplate.pretty_repr()\\nImagePromptTemplate.save()\\nImagePromptTemplate.schema()\\nImagePromptTemplate.schema_json()\\nImagePromptTemplate.stream()\\nImagePromptTemplate.to_json()\\nImagePromptTemplate.to_json_not_implemented()\\nImagePromptTemplate.transform()\\nImagePromptTemplate.update_forward_refs()\\nImagePromptTemplate.validate()\\nImagePromptTemplate.with_config()\\nImagePromptTemplate.with_fallbacks()\\nImagePromptTemplate.with_listeners()\\nImagePromptTemplate.with_retry()\\nImagePromptTemplate.with_types()\\nImagePromptTemplate.InputType\\nImagePromptTemplate.OutputType\\nImagePromptTemplate.config_specs\\nImagePromptTemplate.input_schema\\nImagePromptTemplate.lc_attributes\\nImagePromptTemplate.lc_secrets\\nImagePromptTemplate.name\\nImagePromptTemplate.output_schema\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlangchain_core.prompts.image.ImagePromptTemplate¬∂\\n\\n\\nclass langchain_core.prompts.image.ImagePromptTemplate[source]¬∂\\nBases: BasePromptTemplate[ImageURL]\\nImage prompt template for a multimodal model.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\n\\n\\nparam input_types: Dict[str, Any] [Optional]¬∂\\nA dictionary of the types of the variables the prompt template expects.\\nIf not provided, all variables are assumed to be strings.\\n\\n\\n\\nparam input_variables: List[str] [Required]¬∂\\nA list of the names of the variables the prompt template expects.\\n\\n\\n\\nparam metadata: Optional[Dict[str, Any]] = None¬∂\\nMetadata to be used for tracing.\\n\\n\\n\\nparam output_parser: Optional[BaseOutputParser] = None¬∂\\nHow to parse the output of calling an LLM on this formatted prompt.', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'}),\n",
       " Document(page_content='param output_parser: Optional[BaseOutputParser] = None¬∂\\nHow to parse the output of calling an LLM on this formatted prompt.\\n\\n\\n\\nparam partial_variables: Mapping[str, Any] [Optional]¬∂\\nA dictionary of the partial variables the prompt template carries.\\nPartial variables populate the template so that you don‚Äôt need to\\npass them in every time you call the prompt.\\n\\n\\n\\nparam tags: Optional[List[str]] = None¬∂\\nTags to be used for tracing.\\n\\n\\n\\nparam template: dict [Optional]¬∂\\nTemplate for the prompt.\\n\\n\\n\\nasync abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí List[Output]¬∂\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nParameters\\n\\ninputs (List[Input]) ‚Äì \\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) ‚Äì \\nreturn_exceptions (bool) ‚Äì \\nkwargs (Optional[Any]) ‚Äì \\n\\n\\nReturn type\\nList[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí AsyncIterator[Tuple[int, Union[Output, Exception]]]¬∂\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters\\n\\ninputs (List[Input]) ‚Äì \\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) ‚Äì \\nreturn_exceptions (bool) ‚Äì \\nkwargs (Optional[Any]) ‚Äì \\n\\n\\nReturn type\\nAsyncIterator[Tuple[int, Union[Output, Exception]]]\\n\\n\\n\\n\\n\\nasync aformat(**kwargs: Any) ‚Üí ImageURL[source]¬∂\\nFormat the prompt with the inputs.\\n\\nParameters\\nkwargs (Any) ‚Äì Any arguments to be passed to the prompt template.\\n\\nReturns\\nA formatted string.\\n\\nReturn type\\nImageURL\\n\\n\\nExample:\\nawait prompt.aformat(variable1=\"foo\")\\n\\n\\n\\n\\n\\nasync aformat_prompt(**kwargs: Any) ‚Üí PromptValue[source]¬∂\\nCreate Prompt Value.\\n\\nParameters\\nkwargs (Any) ‚Äì \\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nasync ainvoke(input: Dict, config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí PromptValue¬∂\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters\\n\\ninput (Dict) ‚Äì \\nconfig (Optional[RunnableConfig]) ‚Äì \\nkwargs (Any) ‚Äì \\n\\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nassign(**kwargs: Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]) ‚Üí RunnableSerializable[Any, Any]¬∂\\nAssigns new fields to the dict output of this runnable.\\nReturns a new runnable.\\nfrom langchain_community.llms.fake import FakeStreamingListLLM\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import SystemMessagePromptTemplate\\nfrom langchain_core.runnables import Runnable\\nfrom operator import itemgetter\\n\\nprompt = (\\n    SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\\n    + \"{question}\"\\n)\\nllm = FakeStreamingListLLM(responses=[\"foo-lish\"])\\n\\nchain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\\n\\nchain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\\n\\nprint(chain_with_assign.input_schema.schema())\\n# {\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'question\\': {\\'title\\': \\'Question\\', \\'type\\': \\'string\\'}}}\\nprint(chain_with_assign.output_schema.schema()) #\\n{\\'title\\': \\'RunnableSequenceOutput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'str\\': {\\'title\\': \\'Str\\',\\n\\'type\\': \\'string\\'}, \\'hello\\': {\\'title\\': \\'Hello\\', \\'type\\': \\'string\\'}}}\\n\\n\\n\\nParameters\\nkwargs (Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]) ‚Äì', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'}),\n",
       " Document(page_content='model = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\nParameters\\nkwargs (Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) ‚Äì \\n\\nReturn type\\nRunnableSerializable[Input, Output]\\n\\n\\n\\n\\n\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) ‚Üí Model¬∂\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‚Äòallow‚Äô was set since it adds all passed values\\n\\nParameters\\n\\n_fields_set (Optional[SetStr]) ‚Äì \\nvalues (Any) ‚Äì \\n\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) ‚Üí Model¬∂\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\n\\nParameters\\n\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) ‚Äì fields to include in new model\\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) ‚Äì fields to exclude from new model, as with values this takes precedence over include\\nupdate (Optional[DictStrAny]) ‚Äì values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep (bool) ‚Äì set to True to make a deep copy of the model\\nself (Model) ‚Äì \\n\\n\\nReturns\\nnew model instance\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\ndict(**kwargs: Any) ‚Üí Dict¬∂\\nReturn dictionary representation of prompt.\\n\\nParameters\\nkwargs (Any) ‚Äì \\n\\nReturn type\\nDict\\n\\n\\n\\n\\n\\nformat(**kwargs: Any) ‚Üí ImageURL[source]¬∂\\nFormat the prompt with the inputs.\\n\\nParameters\\nkwargs (Any) ‚Äì Any arguments to be passed to the prompt template.\\n\\nReturns\\nA formatted string.\\n\\nReturn type\\nImageURL\\n\\n\\nExample\\nprompt.format(variable1=\"foo\")\\n\\n\\n\\n\\n\\nformat_prompt(**kwargs: Any) ‚Üí PromptValue[source]¬∂\\nCreate Prompt Value.\\n\\nParameters\\nkwargs (Any) ‚Äì \\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nclassmethod from_orm(obj: Any) ‚Üí Model¬∂\\n\\nParameters\\nobj (Any) ‚Äì \\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\nget_graph(config: Optional[RunnableConfig] = None) ‚Üí Graph¬∂\\nReturn a graph representation of this runnable.\\n\\nParameters\\nconfig (Optional[RunnableConfig]) ‚Äì \\n\\nReturn type\\nGraph\\n\\n\\n\\n\\n\\nget_input_schema(config: Optional[RunnableConfig] = None) ‚Üí Type[BaseModel]¬∂\\nGet a pydantic model that can be used to validate input to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic input schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an input schema for a specific configuration.\\n\\nParameters\\nconfig (Optional[RunnableConfig]) ‚Äì A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate input.\\n\\nReturn type\\nType[BaseModel]\\n\\n\\n\\n\\n\\nclassmethod get_lc_namespace() ‚Üí List[str][source]¬∂\\nGet the namespace of the langchain object.\\n\\nReturn type\\nList[str]\\n\\n\\n\\n\\n\\nget_name(suffix: Optional[str] = None, *, name: Optional[str] = None) ‚Üí str¬∂\\nGet the name of the runnable.\\n\\nParameters\\n\\nsuffix (Optional[str]) ‚Äì \\nname (Optional[str]) ‚Äì \\n\\n\\nReturn type\\nstr', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'}),\n",
       " Document(page_content='Parameters\\n\\nb (Union[str, bytes]) ‚Äì \\ncontent_type (unicode) ‚Äì \\nencoding (unicode) ‚Äì \\nproto (Protocol) ‚Äì \\nallow_pickle (bool) ‚Äì \\n\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\npartial(**kwargs: Union[str, Callable[[], str]]) ‚Üí BasePromptTemplate¬∂\\nReturn a partial of the prompt template.\\n\\nParameters\\nkwargs (Union[str, Callable[[], str]]) ‚Äì \\n\\nReturn type\\nBasePromptTemplate\\n\\n\\n\\n\\n\\npick(keys: Union[str, List[str]]) ‚Üí RunnableSerializable[Any, Any]¬∂\\nPick keys from the dict output of this runnable.\\n\\nPick single key:import json\\n\\nfrom langchain_core.runnables import RunnableLambda, RunnableMap\\n\\nas_str = RunnableLambda(str)\\nas_json = RunnableLambda(json.loads)\\nchain = RunnableMap(str=as_str, json=as_json)\\n\\nchain.invoke(\"[1, 2, 3]\")\\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\\n\\njson_only_chain = chain.pick(\"json\")\\njson_only_chain.invoke(\"[1, 2, 3]\")\\n# -> [1, 2, 3]\\n\\n\\n\\nPick list of keys:from typing import Any\\n\\nimport json\\n\\nfrom langchain_core.runnables import RunnableLambda, RunnableMap\\n\\nas_str = RunnableLambda(str)\\nas_json = RunnableLambda(json.loads)\\ndef as_bytes(x: Any) -> bytes:\\n    return bytes(x, \"utf-8\")\\n\\nchain = RunnableMap(\\n    str=as_str,\\n    json=as_json,\\n    bytes=RunnableLambda(as_bytes)\\n)\\n\\nchain.invoke(\"[1, 2, 3]\")\\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\\n\\njson_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\\njson_and_bytes_chain.invoke(\"[1, 2, 3]\")\\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\\n\\n\\n\\n\\n\\nParameters\\nkeys (Union[str, List[str]]) ‚Äì \\n\\nReturn type\\nRunnableSerializable[Any, Any]\\n\\n\\n\\n\\n\\npipe(*others: Union[Runnable[Any, Other], Callable[[Any], Other]], name: Optional[str] = None) ‚Üí RunnableSerializable[Input, Other]¬∂\\nCompose this Runnable with Runnable-like objects to make a RunnableSequence.\\nEquivalent to RunnableSequence(self, *others) or self | others[0] | ‚Ä¶\\nExample\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef add_one(x: int) -> int:\\n    return x + 1\\n\\ndef mul_two(x: int) -> int:\\n    return x * 2\\n\\nrunnable_1 = RunnableLambda(add_one)\\nrunnable_2 = RunnableLambda(mul_two)\\nsequence = runnable_1.pipe(runnable_2)\\n# Or equivalently:\\n# sequence = runnable_1 | runnable_2\\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\\nsequence.invoke(1)\\nawait sequence.ainvoke(1)\\n# -> 4\\n\\nsequence.batch([1, 2, 3])\\nawait sequence.abatch([1, 2, 3])\\n# -> [4, 6, 8]\\n\\n\\n\\nParameters\\n\\nothers (Union[Runnable[Any, Other], Callable[[Any], Other]]) ‚Äì \\nname (Optional[str]) ‚Äì \\n\\n\\nReturn type\\nRunnableSerializable[Input, Other]\\n\\n\\n\\n\\n\\npretty_repr(html: bool = False) ‚Üí str[source]¬∂\\n\\nParameters\\nhtml (bool) ‚Äì \\n\\nReturn type\\nstr\\n\\n\\n\\n\\n\\nsave(file_path: Union[Path, str]) ‚Üí None¬∂\\nSave the prompt.\\n\\nParameters\\nfile_path (Union[Path, str]) ‚Äì Path to directory to save prompt to.\\n\\nReturn type\\nNone\\n\\n\\nExample:\\n.. code-block:: python\\n\\nprompt.save(file_path=‚Äùpath/prompt.yaml‚Äù)\\n\\n\\n\\n\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = \\'#/definitions/{model}\\') ‚Üí DictStrAny¬∂\\n\\nParameters\\n\\nby_alias (bool) ‚Äì \\nref_template (unicode) ‚Äì \\n\\n\\nReturn type\\nDictStrAny\\n\\n\\n\\n\\n\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = \\'#/definitions/{model}\\', **dumps_kwargs: Any) ‚Üí unicode¬∂\\n\\nParameters\\n\\nby_alias (bool) ‚Äì \\nref_template (unicode) ‚Äì \\ndumps_kwargs (Any) ‚Äì \\n\\n\\nReturn type\\nunicode\\n\\n\\n\\n\\n\\nstream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí Iterator[Output]¬∂\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters\\n\\ninput (Input) ‚Äì \\nconfig (Optional[RunnableConfig]) ‚Äì \\nkwargs (Optional[Any]) ‚Äì \\n\\n\\nReturn type\\nIterator[Output]\\n\\n\\n\\n\\n\\nto_json() ‚Üí Union[SerializedConstructor, SerializedNotImplemented]¬∂\\nSerialize the runnable to JSON.\\n\\nReturn type\\nUnion[SerializedConstructor, SerializedNotImplemented]\\n\\n\\n\\n\\n\\nto_json_not_implemented() ‚Üí SerializedNotImplemented¬∂\\n\\nReturn type\\nSerializedNotImplemented', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Does LangChain accept image prompt?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Does LangChain accept image prompt?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how with example',\n",
       " 'context': [Document(page_content='langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain\\n\\n\\nCore\\n\\n\\nCommunity\\n\\n\\nExperimental\\n\\n\\nText splitters\\n\\n\\nai21\\n\\n\\nairbyte\\n\\n\\nanthropic\\n\\n\\nastradb\\n\\n\\nchroma\\n\\n\\ncohere\\n\\n\\nelasticsearch\\n\\n\\nexa\\n\\n\\nfireworks\\n\\n\\ngoogle-genai\\n\\n\\ngoogle-vertexai\\n\\n\\ngroq\\n\\n\\nibm\\n\\n\\nmistralai\\n\\n\\nmongodb\\n\\n\\nnomic\\n\\n\\nnvidia-ai-endpoints\\n\\n\\nnvidia-trt\\n\\n\\nopenai\\n\\n\\npinecone\\n\\n\\npostgres\\n\\n\\nrobocorp\\n\\n\\ntogether\\n\\n\\nupstage\\n\\n\\nvoyageai\\n\\n\\nPartner libs\\n\\nai21\\nairbyte\\nanthropic\\nastradb\\nchroma\\ncohere\\nelasticsearch\\nexa\\nfireworks\\ngoogle-genai\\ngoogle-vertexai\\ngroq\\nibm\\nmistralai\\nmongodb\\nnomic\\nnvidia-ai-endpoints\\nnvidia-trt\\nopenai\\npinecone\\npostgres\\nrobocorp\\ntogether\\nupstage\\nvoyageai\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle Menu\\n\\n\\n\\nPrev\\nUp\\nNext\\n\\n\\n\\nlangchain_core.prompts.image.ImagePromptTemplate\\nImagePromptTemplate\\nImagePromptTemplate.input_types\\nImagePromptTemplate.input_variables\\nImagePromptTemplate.metadata\\nImagePromptTemplate.output_parser\\nImagePromptTemplate.partial_variables\\nImagePromptTemplate.tags\\nImagePromptTemplate.template\\nImagePromptTemplate.abatch()\\nImagePromptTemplate.abatch_as_completed()\\nImagePromptTemplate.aformat()\\nImagePromptTemplate.aformat_prompt()\\nImagePromptTemplate.ainvoke()\\nImagePromptTemplate.assign()\\nImagePromptTemplate.astream()\\nImagePromptTemplate.astream_events()\\nImagePromptTemplate.astream_log()\\nImagePromptTemplate.atransform()\\nImagePromptTemplate.batch()\\nImagePromptTemplate.batch_as_completed()\\nImagePromptTemplate.bind()\\nImagePromptTemplate.config_schema()\\nImagePromptTemplate.configurable_alternatives()\\nImagePromptTemplate.configurable_fields()\\nImagePromptTemplate.construct()\\nImagePromptTemplate.copy()\\nImagePromptTemplate.dict()\\nImagePromptTemplate.format()\\nImagePromptTemplate.format_prompt()\\nImagePromptTemplate.from_orm()\\nImagePromptTemplate.get_graph()\\nImagePromptTemplate.get_input_schema()\\nImagePromptTemplate.get_lc_namespace()\\nImagePromptTemplate.get_name()\\nImagePromptTemplate.get_output_schema()\\nImagePromptTemplate.get_prompts()\\nImagePromptTemplate.invoke()\\nImagePromptTemplate.is_lc_serializable()\\nImagePromptTemplate.json()\\nImagePromptTemplate.lc_id()\\nImagePromptTemplate.map()\\nImagePromptTemplate.parse_file()\\nImagePromptTemplate.parse_obj()\\nImagePromptTemplate.parse_raw()\\nImagePromptTemplate.partial()\\nImagePromptTemplate.pick()\\nImagePromptTemplate.pipe()\\nImagePromptTemplate.pretty_repr()\\nImagePromptTemplate.save()\\nImagePromptTemplate.schema()\\nImagePromptTemplate.schema_json()\\nImagePromptTemplate.stream()\\nImagePromptTemplate.to_json()\\nImagePromptTemplate.to_json_not_implemented()\\nImagePromptTemplate.transform()\\nImagePromptTemplate.update_forward_refs()\\nImagePromptTemplate.validate()\\nImagePromptTemplate.with_config()\\nImagePromptTemplate.with_fallbacks()\\nImagePromptTemplate.with_listeners()\\nImagePromptTemplate.with_retry()\\nImagePromptTemplate.with_types()\\nImagePromptTemplate.InputType\\nImagePromptTemplate.OutputType\\nImagePromptTemplate.config_specs\\nImagePromptTemplate.input_schema\\nImagePromptTemplate.lc_attributes\\nImagePromptTemplate.lc_secrets\\nImagePromptTemplate.name\\nImagePromptTemplate.output_schema\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlangchain_core.prompts.image.ImagePromptTemplate¬∂\\n\\n\\nclass langchain_core.prompts.image.ImagePromptTemplate[source]¬∂\\nBases: BasePromptTemplate[ImageURL]\\nImage prompt template for a multimodal model.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\n\\n\\nparam input_types: Dict[str, Any] [Optional]¬∂\\nA dictionary of the types of the variables the prompt template expects.\\nIf not provided, all variables are assumed to be strings.\\n\\n\\n\\nparam input_variables: List[str] [Required]¬∂\\nA list of the names of the variables the prompt template expects.\\n\\n\\n\\nparam metadata: Optional[Dict[str, Any]] = None¬∂\\nMetadata to be used for tracing.\\n\\n\\n\\nparam output_parser: Optional[BaseOutputParser] = None¬∂\\nHow to parse the output of calling an LLM on this formatted prompt.', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'}),\n",
       "  Document(page_content='param output_parser: Optional[BaseOutputParser] = None¬∂\\nHow to parse the output of calling an LLM on this formatted prompt.\\n\\n\\n\\nparam partial_variables: Mapping[str, Any] [Optional]¬∂\\nA dictionary of the partial variables the prompt template carries.\\nPartial variables populate the template so that you don‚Äôt need to\\npass them in every time you call the prompt.\\n\\n\\n\\nparam tags: Optional[List[str]] = None¬∂\\nTags to be used for tracing.\\n\\n\\n\\nparam template: dict [Optional]¬∂\\nTemplate for the prompt.\\n\\n\\n\\nasync abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí List[Output]¬∂\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nParameters\\n\\ninputs (List[Input]) ‚Äì \\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) ‚Äì \\nreturn_exceptions (bool) ‚Äì \\nkwargs (Optional[Any]) ‚Äì \\n\\n\\nReturn type\\nList[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí AsyncIterator[Tuple[int, Union[Output, Exception]]]¬∂\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters\\n\\ninputs (List[Input]) ‚Äì \\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) ‚Äì \\nreturn_exceptions (bool) ‚Äì \\nkwargs (Optional[Any]) ‚Äì \\n\\n\\nReturn type\\nAsyncIterator[Tuple[int, Union[Output, Exception]]]\\n\\n\\n\\n\\n\\nasync aformat(**kwargs: Any) ‚Üí ImageURL[source]¬∂\\nFormat the prompt with the inputs.\\n\\nParameters\\nkwargs (Any) ‚Äì Any arguments to be passed to the prompt template.\\n\\nReturns\\nA formatted string.\\n\\nReturn type\\nImageURL\\n\\n\\nExample:\\nawait prompt.aformat(variable1=\"foo\")\\n\\n\\n\\n\\n\\nasync aformat_prompt(**kwargs: Any) ‚Üí PromptValue[source]¬∂\\nCreate Prompt Value.\\n\\nParameters\\nkwargs (Any) ‚Äì \\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nasync ainvoke(input: Dict, config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí PromptValue¬∂\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters\\n\\ninput (Dict) ‚Äì \\nconfig (Optional[RunnableConfig]) ‚Äì \\nkwargs (Any) ‚Äì \\n\\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nassign(**kwargs: Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]) ‚Üí RunnableSerializable[Any, Any]¬∂\\nAssigns new fields to the dict output of this runnable.\\nReturns a new runnable.\\nfrom langchain_community.llms.fake import FakeStreamingListLLM\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import SystemMessagePromptTemplate\\nfrom langchain_core.runnables import Runnable\\nfrom operator import itemgetter\\n\\nprompt = (\\n    SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\\n    + \"{question}\"\\n)\\nllm = FakeStreamingListLLM(responses=[\"foo-lish\"])\\n\\nchain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\\n\\nchain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\\n\\nprint(chain_with_assign.input_schema.schema())\\n# {\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'question\\': {\\'title\\': \\'Question\\', \\'type\\': \\'string\\'}}}\\nprint(chain_with_assign.output_schema.schema()) #\\n{\\'title\\': \\'RunnableSequenceOutput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'str\\': {\\'title\\': \\'Str\\',\\n\\'type\\': \\'string\\'}, \\'hello\\': {\\'title\\': \\'Hello\\', \\'type\\': \\'string\\'}}}\\n\\n\\n\\nParameters\\nkwargs (Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]) ‚Äì', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'}),\n",
       "  Document(page_content='model = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\nParameters\\nkwargs (Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) ‚Äì \\n\\nReturn type\\nRunnableSerializable[Input, Output]\\n\\n\\n\\n\\n\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) ‚Üí Model¬∂\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‚Äòallow‚Äô was set since it adds all passed values\\n\\nParameters\\n\\n_fields_set (Optional[SetStr]) ‚Äì \\nvalues (Any) ‚Äì \\n\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) ‚Üí Model¬∂\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\n\\nParameters\\n\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) ‚Äì fields to include in new model\\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) ‚Äì fields to exclude from new model, as with values this takes precedence over include\\nupdate (Optional[DictStrAny]) ‚Äì values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep (bool) ‚Äì set to True to make a deep copy of the model\\nself (Model) ‚Äì \\n\\n\\nReturns\\nnew model instance\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\ndict(**kwargs: Any) ‚Üí Dict¬∂\\nReturn dictionary representation of prompt.\\n\\nParameters\\nkwargs (Any) ‚Äì \\n\\nReturn type\\nDict\\n\\n\\n\\n\\n\\nformat(**kwargs: Any) ‚Üí ImageURL[source]¬∂\\nFormat the prompt with the inputs.\\n\\nParameters\\nkwargs (Any) ‚Äì Any arguments to be passed to the prompt template.\\n\\nReturns\\nA formatted string.\\n\\nReturn type\\nImageURL\\n\\n\\nExample\\nprompt.format(variable1=\"foo\")\\n\\n\\n\\n\\n\\nformat_prompt(**kwargs: Any) ‚Üí PromptValue[source]¬∂\\nCreate Prompt Value.\\n\\nParameters\\nkwargs (Any) ‚Äì \\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nclassmethod from_orm(obj: Any) ‚Üí Model¬∂\\n\\nParameters\\nobj (Any) ‚Äì \\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\nget_graph(config: Optional[RunnableConfig] = None) ‚Üí Graph¬∂\\nReturn a graph representation of this runnable.\\n\\nParameters\\nconfig (Optional[RunnableConfig]) ‚Äì \\n\\nReturn type\\nGraph\\n\\n\\n\\n\\n\\nget_input_schema(config: Optional[RunnableConfig] = None) ‚Üí Type[BaseModel]¬∂\\nGet a pydantic model that can be used to validate input to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic input schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an input schema for a specific configuration.\\n\\nParameters\\nconfig (Optional[RunnableConfig]) ‚Äì A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate input.\\n\\nReturn type\\nType[BaseModel]\\n\\n\\n\\n\\n\\nclassmethod get_lc_namespace() ‚Üí List[str][source]¬∂\\nGet the namespace of the langchain object.\\n\\nReturn type\\nList[str]\\n\\n\\n\\n\\n\\nget_name(suffix: Optional[str] = None, *, name: Optional[str] = None) ‚Üí str¬∂\\nGet the name of the runnable.\\n\\nParameters\\n\\nsuffix (Optional[str]) ‚Äì \\nname (Optional[str]) ‚Äì \\n\\n\\nReturn type\\nstr', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'}),\n",
       "  Document(page_content='Returns\\nA new Runnable that retries the original runnable on exceptions.\\n\\nReturn type\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None) ‚Üí Runnable[Input, Output]¬∂\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters\\n\\ninput_type (Optional[Type[Input]]) ‚Äì \\noutput_type (Optional[Type[Output]]) ‚Äì \\n\\n\\nReturn type\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nproperty InputType: Type[Input]¬∂\\nThe type of input this runnable accepts specified as a type annotation.\\n\\n\\n\\nproperty OutputType: Any¬∂\\nThe type of output this runnable produces specified as a type annotation.\\n\\n\\n\\nproperty config_specs: List[ConfigurableFieldSpec]¬∂\\nList configurable fields for this runnable.\\n\\n\\n\\nproperty input_schema: Type[BaseModel]¬∂\\nThe type of input this runnable accepts specified as a pydantic model.\\n\\n\\n\\nproperty lc_attributes: Dict¬∂\\nList of attribute names that should be included in the serialized kwargs.\\nThese attributes must be accepted by the constructor.\\n\\n\\n\\nproperty lc_secrets: Dict[str, str]¬∂\\nA map of constructor argument names to secret ids.\\n\\nFor example,{‚Äúopenai_api_key‚Äù: ‚ÄúOPENAI_API_KEY‚Äù}\\n\\n\\n\\n\\n\\nname: Optional[str] = None¬∂\\nThe name of the runnable. Used for debugging and tracing.\\n\\n\\n\\nproperty output_schema: Type[BaseModel]¬∂\\nThe type of output this runnable produces specified as a pydantic model.\\n\\n\\n\\n\\n\\n\\n\\n\\n            ¬© 2023, LangChain, Inc..\\n          Last updated on May 05, 2024.', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate ‚Äî ü¶úüîó LangChain 0.1.17', 'language': 'en'})],\n",
       " 'answer': 'The `ImagePromptTemplate` in LangChain is specifically designed to handle image prompts for multimodal models. Here\\'s how you can use it with an example:\\n\\n### Step 1: Define Your Image Prompt Template\\n\\nFirst, you need to create an instance of `ImagePromptTemplate`. This template will be used to format the image URL into a prompt that can be understood by a multimodal model (like an LLM that can process both text and images).\\n\\n```python\\nfrom langchain_core.prompts.image import ImagePromptTemplate\\n\\n# Create an instance of ImagePromptTemplate\\nimage_prompt = ImagePromptTemplate(template={\"image\": \"{image_url}\"})\\n```\\n\\n### Step 2: Formatting the Prompt\\n\\nYou will need to format the prompt with the actual image URL you want to use.\\n\\n```python\\n# Format the prompt with an actual image URL\\nformatted_prompt = image_prompt.format(image_url=\"https://example.com/path/to/your/image.jpg\")\\nprint(formatted_prompt)\\n```\\n\\n### Step 3: Using the Prompt with a Multimodal Model\\n\\nAssuming you have a multimodal model that can take this formatted prompt (image URL in this case), you would typically pass this formatted prompt to the model. Here‚Äôs a conceptual example of how this might look:\\n\\n```python\\n# Assuming \\'multimodal_model\\' is your multimodal LLM that can process image prompts\\nresponse = multimodal_model.invoke(input={\"prompt\": formatted_prompt})\\nprint(response)\\n```\\n\\n### Complete Example\\n\\nHere\\'s everything put together in a simple script:\\n\\n```python\\nfrom langchain_core.prompts.image import ImagePromptTemplate\\n\\n# Create an image prompt template\\nimage_prompt = ImagePromptTemplate(template={\"image\": \"{image_url}\"})\\n\\n# Format the prompt with an actual image URL\\nformatted_prompt = image_prompt.format(image_url=\"https://example.com/path/to/your/image.jpg\")\\n\\n# Assuming \\'multimodal_model\\' is a model that can accept image prompts\\n# This is a hypothetical function call, replace it with actual model invocation code\\nresponse = multimodal_model.invoke(input={\"prompt\": formatted_prompt})\\n\\n# Print the model\\'s response\\nprint(response)\\n```\\n\\nIn this example, `multimodal_model` should be replaced with the actual model instance you are using, which should be capable of processing image inputs. This example assumes that the model and its invocation method (`invoke`) are set up to handle such inputs.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how with example\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "tools = []\n",
    "\n",
    "def call_tools(msg: AIMessage) -> Runnable:\n",
    "    \"\"\"Simple sequential tool calling helper.\"\"\"\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    tool_calls = msg.tool_calls.copy()\n",
    "    for tool_call in tool_calls:\n",
    "        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "    return tool_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcockpit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
