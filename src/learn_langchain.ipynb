{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单次对话 demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Hi!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多模态 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "  \n",
    "image_base64 = encode_image(\"../docs/figures/s-3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to be a graphical representation of an advanced driver-assistance system (ADAS). Here's a breakdown of its components:\n",
      "\n",
      "1. **Left Section**: This shows a first-person view from inside a car. We can see the driver's hand on the steering wheel and the dashboard. A heads-up display (HUD) overlay shows a warning symbol indicating another car is approaching from the left, suggesting the driver needs to pay attention.\n",
      "\n",
      "2. **Center Section**: This part of the image has a diagram illustrating the flow of information and actions in the system. It includes sensory inputs from what appears to be a virtual or augmented reality view, analyzed by an ADAS system represented by a robot icon named \"HCopilot\". The robot processes the data, deciding that the driver should watch out for a car from the left.\n",
      "\n",
      "3. **Right Section**: This part of the image offers another external view of the driving environment seen from outside the car, showing a sunny, urban setting with roads and buildings.\n",
      "\n",
      "Overall, the image seems to illustrate how modern technology like ADAS can be used to enhance safety in driving through visual sensors and AI-driven analysis, providing real-time feedback and alerts to drivers.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "            \"detail\": \"auto\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This image appears to showcase a group of individuals associated with an educational or instructional program, divided into categories such as Instructors, Co-Instructors, and Teaching Assistants. Each person is represented by a circular portrait, and their names are displayed below each image. If you have any specific questions about the layout or design of this image, feel free to ask!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    {\"type\": \"text\", \"text\": \"{input}\"},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            # \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "            \"url\": \"https://lsky.metattri.com/i/2024/04/05/660ecef5ef6f2.png\",\n",
    "            # \"url\": \"{img_url}\",\n",
    "            \"detail\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_openai = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": messages\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical expert.\"),\n",
    "    HumanMessage(content=messages),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"What is in this image?\"})\n",
    "# chain.invoke({\"input\": \"What is in this image?\",\n",
    "#               \"img_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "#               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, LangChain does accept image prompts. This is indicated by the presence of the `ImagePromptTemplate` class within the `langchain_core.prompts.image` module, which is specifically designed for creating image prompt templates for a multimodal model.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "input = \"Does LangChain accept image prompt?\"\n",
    "res = retrieval_chain.invoke({\"input\": input})\n",
    "\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多轮对话 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Does LangChain accept image prompt?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Does LangChain accept image prompt?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how with example',\n",
       " 'context': [Document(page_content='param output_parser: Optional[BaseOutputParser] = None¶\\nHow to parse the output of calling an LLM on this formatted prompt.\\n\\n\\n\\nparam partial_variables: Mapping[str, Any] [Optional]¶\\nA dictionary of the partial variables the prompt template carries.\\nPartial variables populate the template so that you don’t need to\\npass them in every time you call the prompt.\\n\\n\\n\\nparam tags: Optional[List[str]] = None¶\\nTags to be used for tracing.\\n\\n\\n\\nparam template: dict [Optional]¶\\nTemplate for the prompt.\\n\\n\\n\\nasync abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) → List[Output]¶\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nParameters\\n\\ninputs (List[Input]) – \\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) – \\nreturn_exceptions (bool) – \\nkwargs (Optional[Any]) – \\n\\n\\nReturn type\\nList[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) → AsyncIterator[Tuple[int, Union[Output, Exception]]]¶\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters\\n\\ninputs (List[Input]) – \\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) – \\nreturn_exceptions (bool) – \\nkwargs (Optional[Any]) – \\n\\n\\nReturn type\\nAsyncIterator[Tuple[int, Union[Output, Exception]]]\\n\\n\\n\\n\\n\\nasync aformat(**kwargs: Any) → ImageURL[source]¶\\nFormat the prompt with the inputs.\\n\\nParameters\\nkwargs (Any) – Any arguments to be passed to the prompt template.\\n\\nReturns\\nA formatted string.\\n\\nReturn type\\nImageURL\\n\\n\\nExample:\\nawait prompt.aformat(variable1=\"foo\")\\n\\n\\n\\n\\n\\nasync aformat_prompt(**kwargs: Any) → PromptValue[source]¶\\nCreate Prompt Value.\\n\\nParameters\\nkwargs (Any) – \\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nasync ainvoke(input: Dict, config: Optional[RunnableConfig] = None, **kwargs: Any) → PromptValue¶\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters\\n\\ninput (Dict) – \\nconfig (Optional[RunnableConfig]) – \\nkwargs (Any) – \\n\\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nassign(**kwargs: Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]) → RunnableSerializable[Any, Any]¶\\nAssigns new fields to the dict output of this runnable.\\nReturns a new runnable.\\nfrom langchain_community.llms.fake import FakeStreamingListLLM\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import SystemMessagePromptTemplate\\nfrom langchain_core.runnables import Runnable\\nfrom operator import itemgetter\\n\\nprompt = (\\n    SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\\n    + \"{question}\"\\n)\\nllm = FakeStreamingListLLM(responses=[\"foo-lish\"])\\n\\nchain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\\n\\nchain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\\n\\nprint(chain_with_assign.input_schema.schema())\\n# {\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'question\\': {\\'title\\': \\'Question\\', \\'type\\': \\'string\\'}}}\\nprint(chain_with_assign.output_schema.schema()) #\\n{\\'title\\': \\'RunnableSequenceOutput\\', \\'type\\': \\'object\\', \\'properties\\':\\n{\\'str\\': {\\'title\\': \\'Str\\',\\n\\'type\\': \\'string\\'}, \\'hello\\': {\\'title\\': \\'Hello\\', \\'type\\': \\'string\\'}}}\\n\\n\\n\\nParameters\\nkwargs (Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]) –', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate — 🦜🔗 LangChain 0.1.16', 'language': 'en'}),\n",
       "  Document(page_content='model = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\nParameters\\nkwargs (Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) – \\n\\nReturn type\\nRunnableSerializable[Input, Output]\\n\\n\\n\\n\\n\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model¶\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\n\\nParameters\\n\\n_fields_set (Optional[SetStr]) – \\nvalues (Any) – \\n\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model¶\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\n\\nParameters\\n\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – fields to include in new model\\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – fields to exclude from new model, as with values this takes precedence over include\\nupdate (Optional[DictStrAny]) – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep (bool) – set to True to make a deep copy of the model\\nself (Model) – \\n\\n\\nReturns\\nnew model instance\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\ndict(**kwargs: Any) → Dict¶\\nReturn dictionary representation of prompt.\\n\\nParameters\\nkwargs (Any) – \\n\\nReturn type\\nDict\\n\\n\\n\\n\\n\\nformat(**kwargs: Any) → ImageURL[source]¶\\nFormat the prompt with the inputs.\\n\\nParameters\\nkwargs (Any) – Any arguments to be passed to the prompt template.\\n\\nReturns\\nA formatted string.\\n\\nReturn type\\nImageURL\\n\\n\\nExample\\nprompt.format(variable1=\"foo\")\\n\\n\\n\\n\\n\\nformat_prompt(**kwargs: Any) → PromptValue[source]¶\\nCreate Prompt Value.\\n\\nParameters\\nkwargs (Any) – \\n\\nReturn type\\nPromptValue\\n\\n\\n\\n\\n\\nclassmethod from_orm(obj: Any) → Model¶\\n\\nParameters\\nobj (Any) – \\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\nget_graph(config: Optional[RunnableConfig] = None) → Graph¶\\nReturn a graph representation of this runnable.\\n\\nParameters\\nconfig (Optional[RunnableConfig]) – \\n\\nReturn type\\nGraph\\n\\n\\n\\n\\n\\nget_input_schema(config: Optional[RunnableConfig] = None) → Type[BaseModel]¶\\nGet a pydantic model that can be used to validate input to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic input schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an input schema for a specific configuration.\\n\\nParameters\\nconfig (Optional[RunnableConfig]) – A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate input.\\n\\nReturn type\\nType[BaseModel]\\n\\n\\n\\n\\n\\nclassmethod get_lc_namespace() → List[str][source]¶\\nGet the namespace of the langchain object.\\n\\nReturn type\\nList[str]\\n\\n\\n\\n\\n\\nget_name(suffix: Optional[str] = None, *, name: Optional[str] = None) → str¶\\nGet the name of the runnable.\\n\\nParameters\\n\\nsuffix (Optional[str]) – \\nname (Optional[str]) – \\n\\n\\nReturn type\\nstr', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate — 🦜🔗 LangChain 0.1.16', 'language': 'en'}),\n",
       "  Document(page_content='langchain_core.prompts.image.ImagePromptTemplate — 🦜🔗 LangChain 0.1.16\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain\\n\\n\\nCore\\n\\n\\nCommunity\\n\\n\\nExperimental\\n\\n\\nText splitters\\n\\n\\nai21\\n\\n\\nairbyte\\n\\n\\nanthropic\\n\\n\\nastradb\\n\\n\\nchroma\\n\\n\\ncohere\\n\\n\\nelasticsearch\\n\\n\\nexa\\n\\n\\nfireworks\\n\\n\\ngoogle-genai\\n\\n\\ngoogle-vertexai\\n\\n\\ngroq\\n\\n\\nibm\\n\\n\\nmistralai\\n\\n\\nmongodb\\n\\n\\nnomic\\n\\n\\nnvidia-ai-endpoints\\n\\n\\nnvidia-trt\\n\\n\\nopenai\\n\\n\\npinecone\\n\\n\\npostgres\\n\\n\\nrobocorp\\n\\n\\ntogether\\n\\n\\nupstage\\n\\n\\nvoyageai\\n\\n\\nPartner libs\\n\\nai21\\nairbyte\\nanthropic\\nastradb\\nchroma\\ncohere\\nelasticsearch\\nexa\\nfireworks\\ngoogle-genai\\ngoogle-vertexai\\ngroq\\nibm\\nmistralai\\nmongodb\\nnomic\\nnvidia-ai-endpoints\\nnvidia-trt\\nopenai\\npinecone\\npostgres\\nrobocorp\\ntogether\\nupstage\\nvoyageai\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle Menu\\n\\n\\n\\nPrev\\nUp\\nNext\\n\\n\\n\\nlangchain_core.prompts.image.ImagePromptTemplate\\nImagePromptTemplate\\nImagePromptTemplate.input_types\\nImagePromptTemplate.input_variables\\nImagePromptTemplate.metadata\\nImagePromptTemplate.output_parser\\nImagePromptTemplate.partial_variables\\nImagePromptTemplate.tags\\nImagePromptTemplate.template\\nImagePromptTemplate.abatch()\\nImagePromptTemplate.abatch_as_completed()\\nImagePromptTemplate.aformat()\\nImagePromptTemplate.aformat_prompt()\\nImagePromptTemplate.ainvoke()\\nImagePromptTemplate.assign()\\nImagePromptTemplate.astream()\\nImagePromptTemplate.astream_events()\\nImagePromptTemplate.astream_log()\\nImagePromptTemplate.atransform()\\nImagePromptTemplate.batch()\\nImagePromptTemplate.batch_as_completed()\\nImagePromptTemplate.bind()\\nImagePromptTemplate.config_schema()\\nImagePromptTemplate.configurable_alternatives()\\nImagePromptTemplate.configurable_fields()\\nImagePromptTemplate.construct()\\nImagePromptTemplate.copy()\\nImagePromptTemplate.dict()\\nImagePromptTemplate.format()\\nImagePromptTemplate.format_prompt()\\nImagePromptTemplate.from_orm()\\nImagePromptTemplate.get_graph()\\nImagePromptTemplate.get_input_schema()\\nImagePromptTemplate.get_lc_namespace()\\nImagePromptTemplate.get_name()\\nImagePromptTemplate.get_output_schema()\\nImagePromptTemplate.get_prompts()\\nImagePromptTemplate.invoke()\\nImagePromptTemplate.is_lc_serializable()\\nImagePromptTemplate.json()\\nImagePromptTemplate.lc_id()\\nImagePromptTemplate.map()\\nImagePromptTemplate.parse_file()\\nImagePromptTemplate.parse_obj()\\nImagePromptTemplate.parse_raw()\\nImagePromptTemplate.partial()\\nImagePromptTemplate.pick()\\nImagePromptTemplate.pipe()\\nImagePromptTemplate.pretty_repr()\\nImagePromptTemplate.save()\\nImagePromptTemplate.schema()\\nImagePromptTemplate.schema_json()\\nImagePromptTemplate.stream()\\nImagePromptTemplate.to_json()\\nImagePromptTemplate.to_json_not_implemented()\\nImagePromptTemplate.transform()\\nImagePromptTemplate.update_forward_refs()\\nImagePromptTemplate.validate()\\nImagePromptTemplate.with_config()\\nImagePromptTemplate.with_fallbacks()\\nImagePromptTemplate.with_listeners()\\nImagePromptTemplate.with_retry()\\nImagePromptTemplate.with_types()\\nImagePromptTemplate.InputType\\nImagePromptTemplate.OutputType\\nImagePromptTemplate.config_specs\\nImagePromptTemplate.input_schema\\nImagePromptTemplate.lc_attributes\\nImagePromptTemplate.lc_secrets\\nImagePromptTemplate.name\\nImagePromptTemplate.output_schema\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlangchain_core.prompts.image.ImagePromptTemplate¶\\n\\n\\nclass langchain_core.prompts.image.ImagePromptTemplate[source]¶\\nBases: BasePromptTemplate[ImageURL]\\nAn image prompt template for a multimodal model.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\n\\n\\nparam input_types: Dict[str, Any] [Optional]¶\\nA dictionary of the types of the variables the prompt template expects.\\nIf not provided, all variables are assumed to be strings.\\n\\n\\n\\nparam input_variables: List[str] [Required]¶\\nA list of the names of the variables the prompt template expects.\\n\\n\\n\\nparam metadata: Optional[Dict[str, Any]] = None¶\\nMetadata to be used for tracing.\\n\\n\\n\\nparam output_parser: Optional[BaseOutputParser] = None¶\\nHow to parse the output of calling an LLM on this formatted prompt.', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate — 🦜🔗 LangChain 0.1.16', 'language': 'en'}),\n",
       "  Document(page_content='Parameters\\n\\nb (Union[str, bytes]) – \\ncontent_type (unicode) – \\nencoding (unicode) – \\nproto (Protocol) – \\nallow_pickle (bool) – \\n\\n\\nReturn type\\nModel\\n\\n\\n\\n\\n\\npartial(**kwargs: Union[str, Callable[[], str]]) → BasePromptTemplate¶\\nReturn a partial of the prompt template.\\n\\nParameters\\nkwargs (Union[str, Callable[[], str]]) – \\n\\nReturn type\\nBasePromptTemplate\\n\\n\\n\\n\\n\\npick(keys: Union[str, List[str]]) → RunnableSerializable[Any, Any]¶\\nPick keys from the dict output of this runnable.\\n\\nPick single key:import json\\n\\nfrom langchain_core.runnables import RunnableLambda, RunnableMap\\n\\nas_str = RunnableLambda(str)\\nas_json = RunnableLambda(json.loads)\\nchain = RunnableMap(str=as_str, json=as_json)\\n\\nchain.invoke(\"[1, 2, 3]\")\\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\\n\\njson_only_chain = chain.pick(\"json\")\\njson_only_chain.invoke(\"[1, 2, 3]\")\\n# -> [1, 2, 3]\\n\\n\\n\\nPick list of keys:from typing import Any\\n\\nimport json\\n\\nfrom langchain_core.runnables import RunnableLambda, RunnableMap\\n\\nas_str = RunnableLambda(str)\\nas_json = RunnableLambda(json.loads)\\ndef as_bytes(x: Any) -> bytes:\\n    return bytes(x, \"utf-8\")\\n\\nchain = RunnableMap(\\n    str=as_str,\\n    json=as_json,\\n    bytes=RunnableLambda(as_bytes)\\n)\\n\\nchain.invoke(\"[1, 2, 3]\")\\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\\n\\njson_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\\njson_and_bytes_chain.invoke(\"[1, 2, 3]\")\\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\\n\\n\\n\\n\\n\\nParameters\\nkeys (Union[str, List[str]]) – \\n\\nReturn type\\nRunnableSerializable[Any, Any]\\n\\n\\n\\n\\n\\npipe(*others: Union[Runnable[Any, Other], Callable[[Any], Other]], name: Optional[str] = None) → RunnableSerializable[Input, Other]¶\\nCompose this Runnable with Runnable-like objects to make a RunnableSequence.\\nEquivalent to RunnableSequence(self, *others) or self | others[0] | …\\nExample\\nfrom langchain_core.runnables import RunnableLambda\\n\\ndef add_one(x: int) -> int:\\n    return x + 1\\n\\ndef mul_two(x: int) -> int:\\n    return x * 2\\n\\nrunnable_1 = RunnableLambda(add_one)\\nrunnable_2 = RunnableLambda(mul_two)\\nsequence = runnable_1.pipe(runnable_2)\\n# Or equivalently:\\n# sequence = runnable_1 | runnable_2\\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\\nsequence.invoke(1)\\nawait sequence.ainvoke(1)\\n# -> 4\\n\\nsequence.batch([1, 2, 3])\\nawait sequence.abatch([1, 2, 3])\\n# -> [4, 6, 8]\\n\\n\\n\\nParameters\\n\\nothers (Union[Runnable[Any, Other], Callable[[Any], Other]]) – \\nname (Optional[str]) – \\n\\n\\nReturn type\\nRunnableSerializable[Input, Other]\\n\\n\\n\\n\\n\\npretty_repr(html: bool = False) → str[source]¶\\n\\nParameters\\nhtml (bool) – \\n\\nReturn type\\nstr\\n\\n\\n\\n\\n\\nsave(file_path: Union[Path, str]) → None¶\\nSave the prompt.\\n\\nParameters\\nfile_path (Union[Path, str]) – Path to directory to save prompt to.\\n\\nReturn type\\nNone\\n\\n\\nExample:\\n.. code-block:: python\\n\\nprompt.save(file_path=”path/prompt.yaml”)\\n\\n\\n\\n\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = \\'#/definitions/{model}\\') → DictStrAny¶\\n\\nParameters\\n\\nby_alias (bool) – \\nref_template (unicode) – \\n\\n\\nReturn type\\nDictStrAny\\n\\n\\n\\n\\n\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = \\'#/definitions/{model}\\', **dumps_kwargs: Any) → unicode¶\\n\\nParameters\\n\\nby_alias (bool) – \\nref_template (unicode) – \\ndumps_kwargs (Any) – \\n\\n\\nReturn type\\nunicode\\n\\n\\n\\n\\n\\nstream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) → Iterator[Output]¶\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters\\n\\ninput (Input) – \\nconfig (Optional[RunnableConfig]) – \\nkwargs (Optional[Any]) – \\n\\n\\nReturn type\\nIterator[Output]\\n\\n\\n\\n\\n\\nto_json() → Union[SerializedConstructor, SerializedNotImplemented]¶\\nSerialize the runnable to JSON.\\n\\nReturn type\\nUnion[SerializedConstructor, SerializedNotImplemented]\\n\\n\\n\\n\\n\\nto_json_not_implemented() → SerializedNotImplemented¶\\n\\nReturn type\\nSerializedNotImplemented', metadata={'source': 'https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.image.ImagePromptTemplate.html', 'title': 'langchain_core.prompts.image.ImagePromptTemplate — 🦜🔗 LangChain 0.1.16', 'language': 'en'})],\n",
       " 'answer': 'LangChain supports image prompts through its `ImagePromptTemplate` class. This class is designed to work with multimodal models that can process image inputs. Here’s an example of how you might set up and use an `ImagePromptTemplate` in LangChain:\\n\\n### Example of Using ImagePromptTemplate\\n\\n1. **Define the Image Prompt Template**: First, you need to define the template that specifies how you want to format your image input for the model.\\n\\n```python\\nfrom langchain_core.prompts.image import ImagePromptTemplate\\n\\n# Create an image prompt template\\nimage_prompt = ImagePromptTemplate(template={\\n    \"image_url\": \"{image_url}\"\\n})\\n```\\n\\n2. **Assign an Output Parser**: If your model returns specific data that needs parsing, you can assign an appropriate output parser.\\n\\n```python\\nfrom langchain_core.output_parsers import JsonOutputParser\\n\\n# Assuming the model returns JSON data\\nimage_prompt = image_prompt.assign(output_parser=JsonOutputParser())\\n```\\n\\n3. **Invoke the Model with an Image URL**: You can now invoke the model by passing an image URL. This is how you would typically use it in an application.\\n\\n```python\\n# Example image URL\\nimage_url = \"https://example.com/path/to/image.jpg\"\\n\\n# Prepare the input dictionary\\ninput_data = {\"image_url\": image_url}\\n\\n# Invoke the model\\nresult = await image_prompt.ainvoke(input_data)\\nprint(result)\\n```\\n\\nThis example assumes you have a running environment that supports asynchronous operations (`await`) and that your model is capable of processing image data from URLs.\\n\\n### Key Points\\n- **Customization**: You can customize the `ImagePromptTemplate` depending on the specifics of the model you are using. For instance, if your model expects different parameters or settings, adjust the template accordingly.\\n- **Integration**: The `ImagePromptTemplate` easily integrates with other components in LangChain, allowing for complex workflows involving image processing and decision-making based on image content.\\n- **Async Support**: LangChain\\'s design supports async operations, which is ideal for handling I/O-bound tasks such as image processing.\\n\\nThis setup provides a basic framework to start with image prompts in LangChain, and you can expand or modify it based on the specific requirements of your application and the capabilities of the underlying models.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how with example\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcockpit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
